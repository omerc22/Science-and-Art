{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAuJ8rcCS25_",
        "outputId": "66f0701d-d0b3-4e67-8e8f-6590bbb2859f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install -q streamlit\n",
        "!pip install -q transformers\n",
        "!pip install -q torch\n",
        "!pip install -q sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PfTiwW39oyXJ"
      },
      "outputs": [],
      "source": [
        "!pip install -q pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q diffusers transformers accelerate # Bunlarƒ± ekledik"
      ],
      "metadata": {
        "id": "mbjwBKhWniRF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPmF3eslU1VC",
        "outputId": "60552fd2-1890-43f6-bb7b-74d7edbbbc5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "# Page Configuration\n",
        "st.set_page_config(page_title=\"AI Creative Assistant\", page_icon=\"ü§ñ\", layout=\"wide\")\n",
        "\n",
        "st.title(\"ü§ñ AI Creative Assistant\")\n",
        "st.caption(\"Powered by Google Flan-T5 & Stable Diffusion\")\n",
        "\n",
        "# Sidebar for Mode Selection\n",
        "# Sidebar'ƒ± varsayƒ±lan olarak a√ßƒ±k tutmaya √ßalƒ±≈üalƒ±m ama kullanƒ±cƒ± manuel a√ßmalƒ±\n",
        "mode = st.sidebar.selectbox(\"Select Mode\", [\"Chat Mode\", \"Art Mode\"])\n",
        "st.sidebar.markdown(\"---\")\n",
        "st.sidebar.write(\"Switch between chatting and image generation.\")\n",
        "\n",
        "# --- MODEL LOADING FUNCTIONS ---\n",
        "\n",
        "@st.cache_resource\n",
        "def load_chat_model():\n",
        "    try:\n",
        "        model_name = \"google/flan-t5-base\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "        if torch.cuda.is_available():\n",
        "            model = model.to('cuda')\n",
        "        return tokenizer, model, None\n",
        "    except Exception as e:\n",
        "        return None, None, str(e)\n",
        "\n",
        "@st.cache_resource\n",
        "def load_image_model():\n",
        "    try:\n",
        "        model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "        )\n",
        "        if torch.cuda.is_available():\n",
        "            pipeline = pipeline.to(\"cuda\")\n",
        "        return pipeline, None\n",
        "    except Exception as e:\n",
        "        return None, str(e)\n",
        "\n",
        "# --- CHAT MODE LOGIC ---\n",
        "if mode == \"Chat Mode\":\n",
        "    st.header(\"üí¨ Chat Mode\")\n",
        "\n",
        "    # Load Chat Model\n",
        "    with st.spinner('Loading Chat Model...'):\n",
        "        tokenizer, chat_model, chat_error = load_chat_model()\n",
        "\n",
        "    if chat_error:\n",
        "        st.error(f\"Error loading chat model: {chat_error}\")\n",
        "        st.stop()\n",
        "\n",
        "    # Chat History\n",
        "    if \"messages\" not in st.session_state:\n",
        "        st.session_state.messages = []\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.markdown(message[\"content\"])\n",
        "\n",
        "    # Chat Input\n",
        "    if prompt := st.chat_input(\"Type your message...\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(prompt)\n",
        "\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"Thinking...\"):\n",
        "                try:\n",
        "                    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "                    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "                    if device == 'cuda':\n",
        "                        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "                    with torch.no_grad():\n",
        "\n",
        "                        outputs = chat_model.generate(\n",
        "                            inputs[\"input_ids\"],\n",
        "                            attention_mask=inputs[\"attention_mask\"],\n",
        "                            max_length=150,\n",
        "                            do_sample=True,\n",
        "                            temperature=0.7,\n",
        "                            top_k=50,\n",
        "                            top_p=0.9,\n",
        "                            no_repeat_ngram_size=2,\n",
        "                            early_stopping=True\n",
        "                        )\n",
        "\n",
        "                    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                    st.markdown(response)\n",
        "                    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "                except Exception as e:\n",
        "                    st.error(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "# --- ART MODE LOGIC ---\n",
        "elif mode == \"Art Mode\":\n",
        "    st.header(\"üé® Art Mode\")\n",
        "    st.info(\"Creating images requires heavy GPU usage. Please be patient.\")\n",
        "\n",
        "    # Load Image Model\n",
        "    with st.spinner('Loading Image Model...'):\n",
        "        image_pipeline, img_error = load_image_model()\n",
        "\n",
        "    if img_error:\n",
        "        st.error(f\"Error loading image model: {img_error}\")\n",
        "        st.stop()\n",
        "\n",
        "    prompt = st.text_input(\"Describe the image you want to generate:\", placeholder=\"e.g. A futuristic city with flying cars, cyberpunk style\")\n",
        "    generate_btn = st.button(\"Generate Image\")\n",
        "\n",
        "    if generate_btn and prompt:\n",
        "        with st.spinner(\"Generating masterpiece... (This may take a moment)\"):\n",
        "            try:\n",
        "                image = image_pipeline(prompt).images[0]\n",
        "                st.image(image, caption=prompt, use_column_width=True)\n",
        "            except Exception as e:\n",
        "                st.error(f\"An error occurred during generation: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGL_-7VmVtYy",
        "outputId": "149c4a13-3014-4bb8-b5ab-66364cbbca6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "136.118.89.39"
          ]
        }
      ],
      "source": [
        "!wget -q -O - https://loca.lt/mytunnelpassword"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EXymr3egY32i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oFlPU-HVJq6",
        "outputId": "66c99880-4f8d-48b6-8712-e8f6be0e8d53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K\u001b[1G\u001b[0JNeed to install the following packages:\n",
            "localtunnel@2.0.2\n",
            "Ok to proceed? (y) \u001b[20G\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://136.118.89.39:8501\u001b[0m\n",
            "\u001b[0m\n",
            "y\n",
            "\n",
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0Kyour url is: https://mean-tips-lick.loca.lt\n",
            "2025-12-01 19:13:15.837955: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764616395.859737    7578 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764616395.866248    7578 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764616395.883755    7578 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764616395.883782    7578 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764616395.883786    7578 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764616395.883789    7578 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "tokenizer_config.json: 2.54kB [00:00, 15.9MB/s]\n",
            "spiece.model: 100% 792k/792k [00:00<00:00, 1.60MB/s]\n",
            "tokenizer.json: 2.42MB [00:00, 77.7MB/s]\n",
            "special_tokens_map.json: 2.20kB [00:00, 18.5MB/s]\n",
            "config.json: 1.40kB [00:00, 11.0MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "model.safetensors: 100% 990M/990M [00:09<00:00, 101MB/s]\n",
            "generation_config.json: 100% 147/147 [00:00<00:00, 1.38MB/s]\n",
            "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "model_index.json: 100% 541/541 [00:00<00:00, 5.69MB/s]\n",
            "Fetching 15 files:   0% 0/15 [00:00<?, ?it/s]\n",
            "preprocessor_config.json: 100% 342/342 [00:00<00:00, 3.08MB/s]\n",
            "Fetching 15 files:   7% 1/15 [00:00<00:02,  5.11it/s]\n",
            "config.json: 100% 617/617 [00:00<00:00, 5.99MB/s]\n",
            "\n",
            "special_tokens_map.json: 100% 472/472 [00:00<00:00, 3.71MB/s]\n",
            "\n",
            "config.json: 4.72kB [00:00, 26.6MB/s]\n",
            "\n",
            "merges.txt: 0.00B [00:00, ?B/s]\u001b[A\n",
            "\n",
            "scheduler_config.json: 100% 308/308 [00:00<00:00, 3.47MB/s]\n",
            "merges.txt: 525kB [00:00, 24.4MB/s]\n",
            "\n",
            "safety_checker/model.safetensors:   0% 0.00/1.22G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "text_encoder/model.safetensors:   0% 0.00/492M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tokenizer_config.json: 100% 806/806 [00:00<00:00, 8.23MB/s]\n",
            "\n",
            "\n",
            "\n",
            "vocab.json: 0.00B [00:00, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "config.json: 100% 743/743 [00:00<00:00, 6.30MB/s]\n",
            "vocab.json: 1.06MB [00:00, 42.5MB/s]\n",
            "\n",
            "\n",
            "\n",
            "config.json: 100% 547/547 [00:00<00:00, 4.63MB/s]\n",
            "\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   0% 0.00/3.44G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:   0% 0.00/335M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "text_encoder/model.safetensors:   0% 62.5k/492M [00:00<1:30:28, 90.7kB/s]\u001b[A\u001b[A\n",
            "\n",
            "text_encoder/model.safetensors:   1% 4.08M/492M [00:00<01:10, 6.94MB/s]  \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   0% 52.5k/3.44G [00:00<13:35:29, 70.3kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "text_encoder/model.safetensors:   2% 8.37M/492M [00:00<00:36, 13.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "text_encoder/model.safetensors:   4% 20.0M/492M [00:01<00:15, 29.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   0% 424k/3.44G [00:00<1:43:20, 554kB/s]   \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:   0% 506k/335M [00:00<10:13, 544kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:   0% 943k/335M [00:01<05:16, 1.05MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "safety_checker/model.safetensors:   0% 454k/1.22G [00:01<1:04:06, 316kB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:   1% 3.65M/335M [00:01<01:24, 3.92MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:   4% 12.2M/335M [00:01<00:24, 13.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "text_encoder/model.safetensors:   5% 24.8M/492M [00:01<00:33, 13.9MB/s]\u001b[A\u001b[A\n",
            "safety_checker/model.safetensors:   0% 522k/1.22G [00:01<1:19:19, 255kB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:  14% 47.5M/335M [00:01<00:04, 57.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:  17% 57.2M/335M [00:01<00:04, 60.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:  23% 76.1M/335M [00:02<00:03, 69.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "text_encoder/model.safetensors:   6% 30.3M/492M [00:02<00:47, 9.68MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:  27% 88.7M/335M [00:02<00:05, 45.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:  35% 116M/335M [00:03<00:04, 52.4MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "safety_checker/model.safetensors:   1% 8.83M/1.22G [00:05<10:32, 1.91MB/s]\u001b[A\n",
            "\n",
            "text_encoder/model.safetensors:   8% 39.9M/492M [00:05<01:18, 5.74MB/s]\u001b[A\u001b[A\n",
            "\n",
            "text_encoder/model.safetensors:  11% 53.0M/492M [00:05<00:46, 9.45MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:  38% 126M/335M [00:05<00:14, 14.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "text_encoder/model.safetensors:  18% 88.9M/492M [00:06<00:18, 22.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "text_encoder/model.safetensors:  28% 140M/492M [00:06<00:07, 45.3MB/s] \u001b[A\u001b[A\n",
            "\n",
            "text_encoder/model.safetensors:  39% 192M/492M [00:06<00:04, 67.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "text_encoder/model.safetensors:  44% 217M/492M [00:07<00:04, 64.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:  41% 136M/335M [00:09<00:25, 7.67MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "text_encoder/model.safetensors:  50% 245M/492M [00:09<00:08, 29.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "text_encoder/model.safetensors:  56% 274M/492M [00:09<00:05, 39.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "text_encoder/model.safetensors:  67% 330M/492M [00:09<00:02, 65.5MB/s]\u001b[A\u001b[A\n",
            "safety_checker/model.safetensors:   1% 17.4M/1.22G [00:10<11:44, 1.70MB/s]\u001b[A\n",
            "\n",
            "text_encoder/model.safetensors:  73% 358M/492M [00:11<00:03, 36.9MB/s]\u001b[A\u001b[A\n",
            "safety_checker/model.safetensors:   7% 84.5M/1.22G [00:12<01:53, 9.97MB/s]\u001b[A\n",
            "safety_checker/model.safetensors:  12% 152M/1.22G [00:12<00:51, 20.8MB/s] \u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   0% 503k/3.44G [00:13<1:43:20, 554kB/s]\u001b[A\u001b[A\u001b[A\n",
            "safety_checker/model.safetensors:  18% 219M/1.22G [00:15<00:44, 22.5MB/s]\u001b[A\n",
            "safety_checker/model.safetensors:  23% 286M/1.22G [00:15<00:26, 35.5MB/s]\u001b[A\n",
            "safety_checker/model.safetensors:  29% 353M/1.22G [00:15<00:16, 52.4MB/s]\u001b[A\n",
            "safety_checker/model.safetensors:  35% 420M/1.22G [00:19<00:25, 30.7MB/s]\u001b[A\n",
            "safety_checker/model.safetensors:  40% 487M/1.22G [00:20<00:18, 38.8MB/s]\u001b[A\n",
            "safety_checker/model.safetensors:  46% 554M/1.22G [00:20<00:12, 54.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   0% 617k/3.44G [00:20<40:27:34, 23.6kB/s]\u001b[A\u001b[A\u001b[A\n",
            "safety_checker/model.safetensors:  51% 621M/1.22G [00:21<00:09, 63.3MB/s]\u001b[A\n",
            "safety_checker/model.safetensors:  57% 688M/1.22G [00:22<00:09, 58.6MB/s]\u001b[A\n",
            "\n",
            "text_encoder/model.safetensors:  73% 358M/492M [00:23<00:03, 36.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:  41% 136M/335M [00:23<00:25, 7.67MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   2% 67.7M/3.44G [00:25<16:03, 3.50MB/s]  \u001b[A\u001b[A\u001b[A\n",
            "safety_checker/model.safetensors:  62% 755M/1.22G [00:26<00:12, 38.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   2% 68.8M/3.44G [00:26<16:01, 3.50MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   4% 136M/3.44G [00:29<07:17, 7.55MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   6% 203M/3.44G [00:30<03:53, 13.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "safety_checker/model.safetensors:  68% 822M/1.22G [00:36<00:25, 15.7MB/s]\u001b[A\n",
            "safety_checker/model.safetensors:  72% 881M/1.22G [00:36<00:15, 21.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   8% 270M/3.44G [00:37<04:26, 11.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  10% 337M/3.44G [00:40<03:34, 14.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "safety_checker/model.safetensors:  78% 948M/1.22G [00:40<00:14, 18.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  12% 404M/3.44G [00:46<03:53, 13.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "text_encoder/model.safetensors:  86% 425M/492M [00:46<00:16, 3.95MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  14% 471M/3.44G [00:50<03:31, 14.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "safety_checker/model.safetensors:  78% 948M/1.22G [00:53<00:14, 18.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  16% 538M/3.44G [00:56<03:45, 12.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "safety_checker/model.safetensors:  83% 1.02G/1.22G [00:56<00:21, 9.16MB/s]\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  18% 605M/3.44G [00:56<02:38, 17.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "safety_checker/model.safetensors:  89% 1.08G/1.22G [00:57<00:10, 12.6MB/s]\u001b[A\n",
            "safety_checker/model.safetensors:  94% 1.15G/1.22G [00:57<00:03, 17.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  20% 673M/3.44G [00:57<01:55, 23.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  22% 740M/3.44G [00:58<01:22, 32.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "safety_checker/model.safetensors: 100% 1.22G/1.22G [00:58<00:00, 20.8MB/s]\n",
            "Fetching 15 files:  27% 4/15 [00:58<02:54, 15.83s/it]\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  23% 807M/3.44G [00:58<01:00, 43.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  25% 874M/3.44G [00:58<00:43, 58.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "text_encoder/model.safetensors: 100% 492M/492M [00:59<00:00, 8.30MB/s]\n",
            "Fetching 15 files:  47% 7/15 [00:59<01:00,  7.51s/it]\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  28% 958M/3.44G [00:59<00:36, 68.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  30% 1.03G/3.44G [00:59<00:27, 86.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  32% 1.09G/3.44G [01:00<00:22, 105MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  34% 1.16G/3.44G [01:00<00:17, 127MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  36% 1.23G/3.44G [01:01<00:21, 101MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  38% 1.29G/3.44G [01:04<00:46, 46.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  40% 1.36G/3.44G [01:04<00:34, 60.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  42% 1.43G/3.44G [01:05<00:28, 71.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  43% 1.49G/3.44G [01:08<00:47, 41.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  45% 1.56G/3.44G [01:09<00:38, 49.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  47% 1.63G/3.44G [01:09<00:27, 64.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  49% 1.70G/3.44G [01:10<00:21, 79.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  51% 1.76G/3.44G [01:14<00:50, 33.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  53% 1.83G/3.44G [01:20<01:17, 20.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  55% 1.90G/3.44G [01:21<00:58, 26.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  57% 1.96G/3.44G [01:25<01:00, 24.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  59% 2.03G/3.44G [01:31<01:18, 17.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  61% 2.10G/3.44G [01:31<00:54, 24.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  63% 2.17G/3.44G [01:35<00:57, 22.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  65% 2.23G/3.44G [01:35<00:41, 28.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  67% 2.30G/3.44G [01:39<00:45, 25.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:  61% 203M/335M [01:39<02:06, 1.05MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  69% 2.37G/3.44G [01:39<00:31, 33.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  71% 2.43G/3.44G [01:40<00:21, 45.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  73% 2.50G/3.44G [01:40<00:15, 60.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  75% 2.57G/3.44G [01:40<00:10, 79.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  77% 2.63G/3.44G [01:45<00:24, 32.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  79% 2.70G/3.44G [01:45<00:16, 45.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  81% 2.77G/3.44G [01:45<00:10, 60.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  82% 2.84G/3.44G [01:46<00:07, 78.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  84% 2.90G/3.44G [01:46<00:05, 99.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  86% 2.97G/3.44G [01:46<00:03, 122MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  88% 3.04G/3.44G [01:46<00:02, 144MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  90% 3.10G/3.44G [01:47<00:02, 152MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:  81% 270M/335M [01:47<00:35, 1.84MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  92% 3.17G/3.44G [01:48<00:02, 108MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  94% 3.24G/3.44G [01:48<00:01, 133MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  96% 3.31G/3.44G [01:51<00:02, 51.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  98% 3.37G/3.44G [01:51<00:00, 71.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors: 100% 3.44G/3.44G [01:55<00:00, 29.7MB/s]\n",
            "Fetching 15 files:  87% 13/15 [01:56<00:17,  8.68s/it]\n",
            "\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors: 100% 335M/335M [01:55<00:00, 2.89MB/s]\n",
            "Fetching 15 files: 100% 15/15 [01:56<00:00,  7.76s/it]\n",
            "Loading pipeline components...:  57% 4/7 [00:16<00:07,  2.60s/it]`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading pipeline components...: 100% 7/7 [00:23<00:00,  3.35s/it]\n",
            "100% 50/50 [00:07<00:00,  6.48it/s]\n",
            "2025-12-01 19:28:19.823 The `use_column_width` parameter has been deprecated and will be removed in a future release. Please utilize the `use_container_width` parameter instead.\n",
            "100% 50/50 [00:06<00:00,  7.36it/s]\n",
            "2025-12-01 19:30:12.119 The `use_column_width` parameter has been deprecated and will be removed in a future release. Please utilize the `use_container_width` parameter instead.\n",
            "100% 50/50 [00:06<00:00,  7.35it/s]\n",
            "2025-12-01 19:30:25.388 The `use_column_width` parameter has been deprecated and will be removed in a future release. Please utilize the `use_container_width` parameter instead.\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}